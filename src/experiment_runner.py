# src/experiment_runner.py
"""
Experiment Runner for Hollow Knight RAG System
Coordinates experiments between RAG system and evaluator - Now with config integration
"""

import json
import time
import os
from typing import List, Dict, Any, Optional
from tqdm import tqdm
import sys
import numpy as np
import re
from transformers import T5ForConditionalGeneration, T5Tokenizer

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from hollow_knight_rag_system import HollowKnightRAGSystem
from prompt_manager import PromptManager, PromptType, create_prompt_comparison_sets
from configs.retriever_configs import get_all_retriever_names
from configs.experiment_configs import (
    EXPERIMENT_SETS,
    get_default_experiment_runner_config,
    validate_config,
    get_experiment_config,
    get_all_experiment_sets
)

# Import for advanced techniques
try:
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("Warning: transformers not available, reranking will be disabled")

class QueryRewriter:
    """
    Implements query rewriting using HyDE (Hypothetical Document Embeddings)
    Uses LLM (google/flan-t5-large) to generate hypothetical documents for better retrieval
    """
    
    def __init__(self, model_name: str = "google/flan-t5-large"):
        self.model_name = model_name
        self.device = "cpu"
        self._setup_model()
    
    def _setup_model(self):
        """Initialize the LLM for query rewriting"""
        try:
            self.tokenizer = T5Tokenizer.from_pretrained(self.model_name)
            self.model = T5ForConditionalGeneration.from_pretrained(self.model_name)
            self.model.to(self.device)
            self.model_loaded = True
            print(f" V Query rewriter initialized with LLM: {self.model_name} on {self.device}")
        except Exception as e:
            print(f" X Failed to initialize query rewriter LLM: {e}")
            self.model_loaded = False
    
    def _generate_hypothetical_document(self, question: str) -> str:
        """
        Use LLM to generate a hypothetical document that answers the question
        
        Args:
            question: Original user question
            
        Returns:
            Hypothetical document generated by LLM
        """
        if not self.model_loaded:
            return ""
        
        try:
            # Create prompt for hypothetical document generation
            prompt = f"""Based on general knowledge about Hollow Knight, generate a comprehensive document that answers the question. The document should Provide detailed information relevant to the question. Include specific examples, locations, characters, or mechanics if applicable. Be written in an informative, encyclopedic style. Contain rich semantic content for better retrieval.

Question: {question}
"""
            
            # Tokenize and generate
            inputs = self.tokenizer.encode(prompt, return_tensors="pt", max_length=512, truncation=True).to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=300,
                    num_beams=3,
                    early_stopping=True,
                    no_repeat_ngram_size=2,
                    temperature=0.7,
                    do_sample=True
                )
            
            hypothetical_doc = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            return hypothetical_doc.strip()
            
        except Exception as e:
            print(f" X Hypothetical document generation failed: {e}")
            return ""
    
    def rewrite_query(self, original_query: str) -> str:
        """
        Rewrite query using HyDE technique - generate hypothetical document with LLM
        
        Args:
            original_query: Original user question
            
        Returns:
            Rewritten query combining original question and hypothetical document
        """
        if not self.model_loaded:
            return original_query  # Fallback to original query
        
        try:
            print(f"   Generating hypothetical document for: '{original_query}'")
            
            # Generate hypothetical document using LLM
            hypothetical_doc = self._generate_hypothetical_document(original_query)
            
            if hypothetical_doc and len(hypothetical_doc) > 50:  # Ensure meaningful content
                # Combine original query with hypothetical document for richer semantics
                rewritten = f"{original_query} {hypothetical_doc}"
                print(f"   Query rewritten with HyDE (length: {len(rewritten)} chars)")
                print(f"   Hypothetical doc preview: {hypothetical_doc[:100]}...")
                return rewritten
            else:
                print(f"   Hypothetical document generation failed or too short, using original query")
                return original_query
            
        except Exception as e:
            print(f" X Query rewriting failed: {e}")
            return original_query

class CrossEncoderReranker:
    """
    Lightweight cross-encoder for document re-ranking
    Re-orders retrieved documents based on query-document relevance
    """
    
    def __init__(self, model_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"):
        self.model_name = model_name
        self._setup_model()
    
    def _setup_model(self):
        """Initialize the cross-encoder model"""
        try:
            if TRANSFORMERS_AVAILABLE:
                print(f"   Loading cross-encoder model: {self.model_name}")
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
                self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)
                
                # Set device
                self.device = "cuda" if torch.cuda.is_available() else "cpu"
                print(f"   Moving model to device: {self.device}")
                self.model.to(self.device)
                self.model.eval()  # Set to evaluation mode
                
                self.model_loaded = True
                print(f" V Cross-encoder reranker initialized: {self.model_name} on {self.device}")
            else:
                self.model_loaded = False
                print(" X Transformers not available, reranking disabled")
        except Exception as e:
            print(f" X Failed to initialize cross-encoder: {e}")
            import traceback
            traceback.print_exc()
            self.model_loaded = False
    
    def rerank_documents(self, query: str, documents: List[str], top_k: int = 5) -> List[str]:
        """
        Re-rank documents based on query-document relevance using cross-encoder
        
        Args:
            query: User question
            documents: List of retrieved documents
            top_k: Number of top documents to return
            
        Returns:
            Re-ranked list of documents
        """
        if not self.model_loaded or not documents:
            return documents[:top_k]  # Fallback to original order
        
        try:
            print(f"   Re-ranking {len(documents)} documents for query: {query}")
            
            # Prepare query-document pairs
            pairs = [(query, doc) for doc in documents]
            
            # Tokenize and get scores
            with torch.no_grad():
                # Tokenize all pairs
                features = self.tokenizer(
                    pairs, 
                    padding=True, 
                    truncation=True, 
                    max_length=512,  # Adjust based on model limits
                    return_tensors="pt"
                ).to(self.device)
                
                # Get model predictions
                scores = self.model(**features).logits.squeeze(-1)
                
                # Convert to probabilities if needed (sigmoid for binary classification)
                scores = torch.sigmoid(scores).cpu().numpy()
            
            # Create list of (score, document) pairs
            scored_docs = list(zip(scores, documents))
            
            # Sort by score in descending order
            scored_docs.sort(key=lambda x: x[0], reverse=True)
            
            # Extract re-ranked documents
            reranked_docs = [doc for score, doc in scored_docs]
            
            print(f"   Re-ranking completed. Top score: {scored_docs[0][0]:.4f}, "
                  f"Bottom score: {scored_docs[-1][0]:.4f}")
            
            return reranked_docs[:top_k]
            
        except Exception as e:
            print(f" X Re-ranking failed: {e}")
            import traceback
            traceback.print_exc()
            return documents[:top_k]  # Fallback on error
class AdvancedHollowKnightRAGSystem(HollowKnightRAGSystem):
    """
    Extended RAG system with advanced techniques: query rewriting and re-ranking
    """
    
    def __init__(self, 
                 base_vector_index_dir: str = "vector_index",
                 retriever_name: str = "bm25",
                 model_name: str = "google/flan-t5-large",
                 prompt_type: PromptType = PromptType.VANILLA_RAG,
                 use_query_rewriting: bool = False,
                 use_reranking: bool = False):
        
        # Initialize parent class
        super().__init__(
            base_vector_index_dir=base_vector_index_dir,
            retriever_name=retriever_name,
            model_name=model_name,
            prompt_type=prompt_type.value
        )
        
        self.use_query_rewriting = use_query_rewriting
        self.use_reranking = use_reranking
        
        # Initialize advanced components if enabled
        if use_query_rewriting:
            self.query_rewriter = QueryRewriter(model_name=model_name)
        
        if use_reranking:
            self.reranker = CrossEncoderReranker()
    
    def query(self, question: str, k: int = 5) -> Dict[str, Any]:
        """
        Enhanced query method with advanced techniques
        
        Args:
            question: User question
            k: Number of documents to use for answer generation (default: 5)
            
        Returns:
            Enhanced response with advanced techniques
        """
        original_question = question
        
        # Step 1: Query Rewriting (if enabled)
        if self.use_query_rewriting and hasattr(self, 'query_rewriter'):
            question = self.query_rewriter.rewrite_query(question)
        
        # Step 2: Initial Retrieval - always retrieve 10 documents
        retrieval_k = k*2  # Fixed number for initial retrieval
        
        try:
            # Retrieve documents using parent's retrieve method
            all_found_docs, retrieved_docs = self.retrieve(question, k=retrieval_k)
            
            # Step 3: Re-ranking (if enabled)
            reranked_docs = []  # This will store re-ranked document dictionaries
            used_reranking = False
            
            if self.use_reranking and hasattr(self, 'reranker'):
                try:
                    if self.reranker.model_loaded:
                        # Extract all document contents for re-ranking
                        all_document_contents = [doc['content'] for doc in all_found_docs]
                        
                        # Re-rank all documents based on content
                        reranked_contents = self.reranker.rerank_documents(
                            original_question, all_document_contents, top_k=retrieval_k
                        )
                        
                        # Reconstruct document dictionaries in re-ranked order
                        # Create a mapping from content to original document
                        content_to_doc = {doc['content']: doc for doc in all_found_docs}
                        reranked_docs = [content_to_doc[content] for content in reranked_contents 
                                    if content in content_to_doc]
                        
                        used_reranking = True
                        print(f"   Re-ranked {len(reranked_docs)} documents")
                        
                    else:
                        print("   Reranker not properly initialized, using original order")
                        reranked_docs = all_found_docs
                        
                except Exception as e:
                    print(f" X Re-ranking failed: {e}")
                    reranked_docs = all_found_docs
            else:
                # No re-ranking, use original documents
                reranked_docs = all_found_docs
            
            # Step 4: Prepare documents for answer generation and evaluation
            if reranked_docs:
                # Use top k documents for answer generation
                docs_for_generation = reranked_docs[:k]
                # Use all reranked documents for evaluation
                docs_for_evaluation = reranked_docs
            else:
                docs_for_generation = []
                docs_for_evaluation = []
            
            # Step 5: Format context and generate answer
            context_for_generation = self.format_context(docs_for_generation)
            prompt = self.build_prompt(original_question, context_for_generation)
            answer = self.generate_answer(prompt) 
            
            # Step 6: Format all documents for evaluation
            all_docs_formatted = self.format_context(docs_for_evaluation)
            
            # Calculate confidence based on retrieval scores
            confidence = 0.0
            if all_found_docs:
                confidence = min(np.mean([doc["score"] for doc in all_found_docs]), 1.0)
            
            # Build the response
            response = {
                "question": original_question,
                "answer": answer,
                "all_docs_formatted": all_docs_formatted,
                "confidence": float(confidence),
                "sources_count": len(docs_for_generation),
                "retriever_used": self.retriever_name,
                "retrieved_documents": all_found_docs  # Keep original documents for reference
            }
            
            # Add advanced technique flags
            response['used_reranking'] = used_reranking
            response['used_query_rewriting'] = self.use_query_rewriting
            response['original_question'] = original_question
            
            if used_reranking:
                response['reranked_documents_count'] = len(reranked_docs)
            
            print(f"   Generated answer using {len(docs_for_generation)} documents")
            
            return response
            
        except Exception as e:
            print(f" X Advanced RAG query failed: {e}")
            # Fallback to parent's full query method
            fallback_response = super().query(original_question, k)
            fallback_response['used_reranking'] = False
            fallback_response['used_query_rewriting'] = self.use_query_rewriting
            fallback_response['original_question'] = original_question
            fallback_response['advanced_query_error'] = str(e)
            return fallback_response

class ClosedBookSystem:
    """
    Closed-book baseline system that uses only the LLM's internal knowledge
    without retrieval for RAG vs Closed-Book comparison
    """
    
    def __init__(self, prompt_type: PromptType = PromptType.CLOSED_BOOK_BASELINE):
        # Initialize PromptManager
        self.prompt_manager = PromptManager()
        self.prompt_type = prompt_type
        self.retriever_used = "none"
        
    def query(self, question: str) -> Dict[str, Any]:
        """
        Generate answer using only LLM's internal knowledge without RAG
        """
        # Use the prompt manager to get the appropriate closed-book prompt
        prompt = self.prompt_manager.get_prompt(self.prompt_type, question)
        
        # In a real implementation, you would call your LLM here
        # For now, simulate a basic closed-book response
        answer = f"Based on my internal knowledge about Hollow Knight: {question}"
        
        return {
            "answer": answer,
            "confidence": 0.5,  # Lower confidence for closed-book
            "sources_count": 0,
            "retriever_used": self.retriever_used
        }

class ExperimentRunner:
    """
    Runs experiments comparing different RAG configurations
    Now integrated with experiment_configs for centralized configuration management
    """
    
    def __init__(self, base_vector_index_dir: str = "vector_index", config: Dict[str, Any] = None):
        self.base_vector_index_dir = base_vector_index_dir
        
        # Load configuration
        if config is None:
            config = get_default_experiment_runner_config()
        self.config = config
        
        # Validate configuration
        self._validate_environment()
    
    def _validate_environment(self):
        """Validate that all required components are available"""
        print("-> Validating experiment environment...")
        errors = validate_config()
        
        if errors:
            print(" X Configuration errors found:")
            for error in errors:
                print(f"  - {error}")
            print(" V Some experiments may fail. Continuing anyway...")
        else:
            print("All configurations are valid")
    
    def _setup_advanced_rag_system(self, retriever_name: str, prompt_type: str, 
                                 use_reranking: bool = False, use_query_rewriting: bool = False):
        """
        Initialize RAG system with advanced features
        
        Args:
            retriever_name: Name of retriever to use
            prompt_type: Type of prompt to use
            use_reranking: Whether to use re-ranking
            use_query_rewriting: Whether to use query rewriting
            
        Returns:
            Initialized RAG system
        """
        try:
            # Convert string prompt_type to PromptType enum
            prompt_type_enum = PromptType(prompt_type)
            
            if use_reranking or use_query_rewriting:
                # Use advanced system
                rag_system = AdvancedHollowKnightRAGSystem(
                    base_vector_index_dir=self.base_vector_index_dir,
                    retriever_name=retriever_name,
                    model_name="google/flan-t5-base",
                    prompt_type=prompt_type_enum,
                    use_query_rewriting=use_query_rewriting,
                    use_reranking=use_reranking
                )
                print(f"   Advanced RAG system initialized with: "
                      f"query_rewriting={use_query_rewriting}, reranking={use_reranking}")
            else:
                # Use basic system
                rag_system = HollowKnightRAGSystem(
                    base_vector_index_dir=self.base_vector_index_dir,
                    retriever_name=retriever_name,
                    model_name="google/flan-t5-base",
                    prompt_type=prompt_type
                )
            
            return rag_system
            
        except Exception as e:
            print(f" X Failed to initialize RAG system: {e}")
            raise
    
    def run_single_configuration(self, 
                               qa_pairs: List[Dict],
                               retriever_name: str,
                               prompt_type: str,
                               experiment_name: str = None,
                               k: int = 5,
                               use_reranking: bool = False,
                               use_query_rewriting: bool = False) -> Dict[str, Any]:
        """
        Run experiment with specific configuration
        
        Args:
            qa_pairs: List of QA pairs to evaluate
            retriever_name: Name of retriever to use
            prompt_type: Type of prompt to use
            experiment_name: Name for this experiment
            k: Number of documents to retrieve
            use_reranking: Whether to use re-ranking
            use_query_rewriting: Whether to use query rewriting
            
        Returns:
            Experiment results
        """
        if experiment_name is None:
            experiment_name = f"{retriever_name}_{prompt_type}"
        
        print(f"-> Running experiment: {experiment_name}")
        print(f"-> Config: retriever={retriever_name}, prompt={prompt_type}, k={k}")
        if use_reranking:
            print("   With re-ranking")
        if use_query_rewriting:
            print("   With query rewriting")
        print(f"   Questions: {len(qa_pairs)}")
        
        # Handle closed-book baseline (no retrieval)
        if retriever_name == "none":
            return self._run_closed_book_experiment(qa_pairs, experiment_name, prompt_type)
        
        # Initialize RAG system with specific configuration
        try:
            rag_system = self._setup_advanced_rag_system(
                retriever_name=retriever_name,
                prompt_type=prompt_type,
                use_reranking=use_reranking,
                use_query_rewriting=use_query_rewriting
            )
        except Exception as e:
            print(f" X Failed to initialize RAG system: {e}")
            return {
                "experiment_name": experiment_name,
                "error": str(e),
                "results": []
            }
        
        results = []
        
        # Process each question
        for qa in tqdm(qa_pairs, desc=f"Processing {experiment_name}"):
            try:
                response = rag_system.query(qa["question"], k=k)
                
                # Build result with advanced features info
                result = {
                    "question_id": qa["question_id"],
                    "question": qa["question"],
                    "generated_answer": response["answer"],
                    "ground_truth": qa["ground_truth_answer"],
                    "all_docs_formatted": response.get("all_docs_formatted", ""),
                    "confidence": response["confidence"],
                    "sources_count": response["sources_count"],
                    "retriever_used": response["retriever_used"],
                    "category": qa.get("category", "unknown"),
                    "difficulty": qa.get("difficulty", "medium"),
                    "config": {
                        "retriever": retriever_name,
                        "prompt_type": prompt_type,
                        "k": k,
                        "use_reranking": use_reranking,
                        "use_query_rewriting": use_query_rewriting
                    }
                }
                
                # Add advanced techniques metadata if used
                if use_query_rewriting:
                    result["original_question"] = response.get("original_question", qa["question"])
                    result["used_query_rewriting"] = response.get("used_query_rewriting", False)
                
                if use_reranking:
                    result["used_reranking"] = response.get("used_reranking", False)
                    if "reranked_documents" in response:
                        result["reranked_docs_count"] = len(response["reranked_documents"])
                
                results.append(result)
                
            except Exception as e:
                print(f" X Error processing {qa['question_id']}: {e}")
                results.append({
                    "question_id": qa["question_id"],
                    "question": qa["question"],
                    "generated_answer": "ERROR",
                    "ground_truth": qa["ground_truth_answer"],
                    "confidence": 0.0,
                    "sources_count": 0,
                    "error": str(e),
                    "config": {
                        "retriever": retriever_name,
                        "prompt_type": prompt_type,
                        "k": k,
                        "use_reranking": use_reranking,
                        "use_query_rewriting": use_query_rewriting
                    }
                })
        
        return {
            "experiment_name": experiment_name,
            "config": {
                "retriever": retriever_name,
                "prompt_type": prompt_type,
                "k": k,
                "use_reranking": use_reranking,
                "use_query_rewriting": use_query_rewriting
            },
            "results": results,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
    
    def _run_closed_book_experiment(self, qa_pairs: List[Dict], experiment_name: str, prompt_type: str) -> Dict[str, Any]:
        """
        Run closed-book baseline experiment (no RAG)
        This fixes the 'Retriever none not found' error
        """
        print("   Running closed-book baseline (no retrieval)...")
        
        try:
            # Convert string prompt_type to PromptType enum
            prompt_type_enum = PromptType(prompt_type)
            closed_book_system = ClosedBookSystem(prompt_type=prompt_type_enum)
        except Exception as e:
            print(f" X Failed to initialize closed-book system: {e}")
            return {
                "experiment_name": experiment_name,
                "error": str(e),
                "results": []
            }
        
        results = []
        
        # Process each question without retrieval
        for qa in tqdm(qa_pairs, desc=f"Processing {experiment_name}"):
            try:
                response = closed_book_system.query(qa["question"])
                
                results.append({
                    "question_id": qa["question_id"],
                    "question": qa["question"],
                    "generated_answer": response["answer"],
                    "ground_truth": qa["ground_truth_answer"],
                    "confidence": response["confidence"],
                    "sources_count": response["sources_count"],
                    "retriever_used": response["retriever_used"],
                    "category": qa.get("category", "unknown"),
                    "difficulty": qa.get("difficulty", "medium"),
                    "config": {
                        "retriever": "none",
                        "prompt_type": prompt_type,
                        "k": 0,
                        "use_reranking": False,
                        "use_query_rewriting": False
                    }
                })
                
            except Exception as e:
                print(f" X Error processing {qa['question_id']}: {e}")
                results.append({
                    "question_id": qa["question_id"],
                    "question": qa["question"],
                    "generated_answer": "ERROR",
                    "ground_truth": qa["ground_truth_answer"],
                    "confidence": 0.0,
                    "sources_count": 0,
                    "error": str(e),
                    "config": {
                        "retriever": "none",
                        "prompt_type": prompt_type,
                        "k": 0
                    }
                })
        
        return {
            "experiment_name": experiment_name,
            "config": {
                "retriever": "none",
                "prompt_type": prompt_type,
                "k": 0,
                "use_reranking": False,
                "use_query_rewriting": False
            },
            "results": results,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
    
    def run_experiment_set(self, experiment_set_name: str, qa_pairs: List[Dict]) -> Dict[str, Any]:
        """
        Run a complete experiment set from configuration
        
        Args:
            experiment_set_name: Name of the experiment set to run
            qa_pairs: List of QA pairs
            
        Returns:
            Experiment set results
        """
        if experiment_set_name not in EXPERIMENT_SETS:
            raise ValueError(f"Unknown experiment set: {experiment_set_name}")
        
        experiment_set = EXPERIMENT_SETS[experiment_set_name]
        print(f"-> Starting {experiment_set['name']}")
        print(f"   {experiment_set['description']}")
        print("=" * 60)
        
        results = {}
        
        # Run each experiment in the set
        for exp_config in experiment_set["experiments"]:
            print(f"\n-> Configuring: {exp_config.name}")
            print(f"   Description: {exp_config.description}")
            
            experiment_result = self.run_single_configuration(
                qa_pairs=qa_pairs,
                retriever_name=exp_config.retriever_name,
                prompt_type=exp_config.prompt_type,
                experiment_name=exp_config.name,
                k=exp_config.k,
                use_reranking=exp_config.use_reranking,
                use_query_rewriting=exp_config.use_query_rewriting
            )
            
            results[exp_config.name] = experiment_result
        
        return {
            "experiment_set": experiment_set_name,
            "set_info": {
                "name": experiment_set["name"],
                "description": experiment_set["description"]
            },
            "individual_results": results
        }
    
    def run_rag_vs_closedbook_comparison(self, qa_pairs: List[Dict]) -> Dict[str, Any]:
        """
        Run RAG vs Closed-Book comparison using configuration
        
        Args:
            qa_pairs: List of QA pairs
            
        Returns:
            Comparison results
        """
        return self.run_experiment_set("rag_vs_closed_book", qa_pairs)
    
    def run_retriever_comparison(self, qa_pairs: List[Dict]) -> Dict[str, Any]:
        """
        Compare different retrievers using configuration
        
        Args:
            qa_pairs: List of QA pairs
            
        Returns:
            Retriever comparison results
        """
        return self.run_experiment_set("retriever_comparison", qa_pairs)
    
    def run_prompt_comparison(self, qa_pairs: List[Dict]) -> Dict[str, Any]:
        """
        Compare different prompt strategies using configuration
        
        Args:
            qa_pairs: List of QA pairs
            
        Returns:
            Prompt comparison results
        """
        return self.run_experiment_set("prompt_comparison", qa_pairs)
    
    def run_advanced_techniques(self, qa_pairs: List[Dict]) -> Dict[str, Any]:
        """
        Run advanced techniques experiments using configuration
        
        Args:
            qa_pairs: List of QA pairs
            
        Returns:
            Advanced techniques results
        """
        return self.run_experiment_set("advanced_techniques", qa_pairs)
    
    def save_results(self, results: Dict, filename: str):
        """
        Save experiment results to file
        
        Args:
            results: Experiment results
            filename: Output filename
        """
        # Create results directory if it doesn't exist
        results_dir = "evaluation/results"
        os.makedirs(results_dir, exist_ok=True)
        
        filepath = os.path.join(results_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f" V Results saved to: {filepath}")
    
    def load_qa_dataset(self, dataset_path: str = "evaluation/hollow_knight_qa_set.json") -> List[Dict]:
        """
        Load QA evaluation dataset
        
        Args:
            dataset_path: Path to QA dataset
            
        Returns:
            List of QA pairs
        """
        try:
            with open(dataset_path, 'r', encoding='utf-8') as f:
                dataset = json.load(f)
            
            print(f" V Loaded QA dataset: {len(dataset['qa_pairs'])} questions")
            return dataset["qa_pairs"]
            
        except Exception as e:
            print(f" X Failed to load QA dataset: {e}")
            return []


def main():
    """Main function to run experiments with configuration integration"""
    print("-> Hollow Knight RAG Experiment Runner - Configuration Integrated")
    print("=" * 70)
    
    # Initialize experiment runner with configuration
    runner = ExperimentRunner()
    
    # Load QA dataset
    qa_pairs = runner.load_qa_dataset()
    if not qa_pairs:
        print(" X No QA pairs loaded. Exiting.")
        return
    
    # Get experiment sets from configuration
    experiment_sets = runner.config.get("experiment_sets", [])
    
    if not experiment_sets:
        print("WARNING: No experiment sets configured. Using default sets.")
        experiment_sets = ["rag_vs_closed_book", "retriever_comparison", "prompt_comparison", "advanced_techniques"]
    
    print(f"-> Configured experiment sets: {', '.join(experiment_sets)}")
    
    # Map experiment set names to methods
    experiment_methods = {
        "rag_vs_closed_book": runner.run_rag_vs_closedbook_comparison,
        "retriever_comparison": runner.run_retriever_comparison,
        "prompt_comparison": runner.run_prompt_comparison,
        "advanced_techniques": runner.run_advanced_techniques
    }
    
    all_results = {}
    
    for exp_set in experiment_sets:
        if exp_set not in experiment_methods:
            print(f"X Unknown experiment set: {exp_set}")
            continue
            
        method = experiment_methods[exp_set]
        exp_set_info = EXPERIMENT_SETS.get(exp_set, {})
        exp_set_name = exp_set_info.get("name", exp_set)
        
        print(f"\n-> Starting {exp_set_name}")
        
        try:
            results = method(qa_pairs)
            all_results[exp_set] = results
            
            # Save individual experiment set results
            runner.save_results(results, f"{exp_set}_results.json")
            
            print(f" V Completed {exp_set_name}")
            
        except Exception as e:
            print(f" X Failed to run {exp_set_name}: {e}")
            import traceback
            traceback.print_exc()
    
    # Save combined results
    if all_results:
        runner.save_results(all_results, "all_experiments_results.json")
        
        # Print summary
        print("\nHAPPY ENDING: All experiments completed successfully!")
        print("-> Experiment Summary:")
        for exp_set, results in all_results.items():
            individual_results = results.get("individual_results", {})
            print(f"  {exp_set}: {len(individual_results)} configurations tested")
            
    else:
        print("\nBAD ENDING: No experiments completed successfully.")


def test_config_integration():
    """Test the configuration integration"""
    print("-> Testing Configuration Integration")
    print("=" * 50)
    
    # Test configuration loading
    config = get_default_experiment_runner_config()
    print(f"Default config: {config}")
    
    # Test available experiment sets
    available_sets = get_all_experiment_sets()
    print(f"Available experiment sets: {available_sets}")
    
    # Test individual experiment config
    for exp_set in available_sets:
        exp_configs = EXPERIMENT_SETS[exp_set]["experiments"]
        print(f"\n{exp_set}: {len(exp_configs)} experiments")
        for exp in exp_configs:
            print(f"  - {exp.name}: {exp.description}")


if __name__ == "__main__":
    # Test configuration integration
    test_config_integration()
    print("\n" + "="*70)
    
    # Run main experiments
    main()